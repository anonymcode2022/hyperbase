{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "820776dc-b5a0-428c-997c-5ae5b0f8e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import *\n",
    "from hyperseed_v2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "386ec038-1e59-4c91-b409-08fdc52ca8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIC= dict(zip(range(1,11),range(0,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b10357be-02ed-4817-9b82-cd219cc314d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "d =2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7479a4ce-1f6f-4141-8f07-76d9cb68174d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 784) (784, 2000)\n"
     ]
    }
   ],
   "source": [
    "from mnist import MNIST\n",
    "import numpy as np\n",
    "mndata = MNIST(\"..\\\\Hyperseed\\\\Data\\\\MNIST\\\\\")\n",
    "np.random.seed(0)\n",
    "imagesTrain, labelsTrain = mndata.load_training()\n",
    "\n",
    "imagesTest, labelsTest = mndata.load_testing()\n",
    " \n",
    "    \n",
    "imagesTrain, labelsTrain = np.array(imagesTrain), np.array(labelsTrain)\n",
    "imagesTest, labelsTest = np.array(imagesTest), np.array(labelsTest)\n",
    "trainSize = 5000\n",
    "testSize = 2000\n",
    "trainImageSet,trainLabelSet = imagesTrain[:trainSize]/255,labelsTrain[:trainSize]\n",
    "inferImageSet, inferLabelSet = imagesTest[:testSize]/255,labelsTest[:testSize]\n",
    "\n",
    "\n",
    "\n",
    "trX,trY = trainImageSet,trainLabelSet\n",
    "teX,teY = inferImageSet,inferLabelSet\n",
    "\n",
    "X,y = trX[:],trY[:]\n",
    "X_test,y_test = teX[:],teY[:]\n",
    "bits = d\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA()\n",
    "compSize=784\n",
    "pca.n_components = compSize\n",
    "model = pca.fit(X)#\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#### Random project data from 784 to dimension d\n",
    "\n",
    "np.random.seed(4) ## 4\n",
    "rnd_project =  2 * np.pi * np.random.rand(compSize, bits) # random angles\n",
    "rnd_project = 1 * np.exp(1j * rnd_project).real # create a phasor vector\n",
    "\n",
    "\n",
    "print(X.shape,rnd_project.shape)\n",
    "TR_D = np.matmul(X,rnd_project)#X#np.matmul(X,rnd_project)\n",
    "TR_D_test=np.matmul(X_test,rnd_project)#X_test#np.matmul(X_test,rnd_project)\n",
    "\n",
    "\n",
    "d = bits\n",
    "\n",
    "### Use fft to make the vectors complex\n",
    "\n",
    "HD_X_shuffeled,y_shuffeled = np.fft.fft(TR_D),y\n",
    "HD_X_shuffeled_test,y_shuffeled_test = np.fft.fft(TR_D_test),y_test\n",
    "\n",
    "\n",
    "\n",
    "numClasses=10\n",
    "num_classes=numClasses\n",
    "classes = [[] for _ in range(numClasses)]\n",
    "classes_test = [[] for _ in range(numClasses)]\n",
    "for i in range(len(HD_X_shuffeled)):\n",
    "    classes[int(y_shuffeled[i])].append(HD_X_shuffeled[i])\n",
    "    \n",
    "for i in range(len(HD_X_shuffeled_test)):\n",
    "    classes_test[int(y_shuffeled_test[i])].append(HD_X_shuffeled_test[i])\n",
    "    \n",
    "    \n",
    "samplesPerClass=[len(i) for i in classes]\n",
    "samplesPerClassTest=[len(i) for i in classes_test]\n",
    "\n",
    "selected_classes= range(10)#[0,1,2,3, 4]# range(10)# [0,1,2,3, 4]\n",
    "TR_perm = np.concatenate(([classes[i] for i in selected_classes]), axis = 0)\n",
    "TR_L_perm = np.concatenate(([[i+1]*samplesPerClass[i] for i in selected_classes]), axis = 0)\n",
    "\n",
    "TE_perm = np.concatenate(([classes_test[i] for i in selected_classes]), axis = 0)\n",
    "TE_L_perm = np.concatenate(([[i+1]*samplesPerClassTest[i] for i in selected_classes]), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e267bb6d-f8b4-48d1-b6ae-399026e60a4e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECTED sample:  4977\n",
      "SELECTED sample:  446\n",
      "SELECTED sample:  1041\n",
      "SELECTED sample:  449\n",
      "SELECTED sample:  3662\n",
      "SELECTED sample:  1196\n",
      "SELECTED sample:  341\n",
      "SELECTED sample:  679\n",
      "SELECTED sample:  2340\n",
      "SELECTED sample:  2555\n",
      "SELECTED sample:  2128\n",
      "SELECTED sample:  2956\n",
      "SELECTED sample:  969\n",
      "SELECTED sample:  2732\n",
      "SELECTED sample:  3412\n",
      "SELECTED sample:  2664\n",
      "SELECTED sample:  3756\n",
      "SELECTED sample:  930\n",
      "SELECTED sample:  2628\n",
      "SELECTED sample:  606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.96      0.87       175\n",
      "           1       0.81      0.96      0.88       234\n",
      "           2       0.78      0.73      0.75       219\n",
      "           3       0.57      0.67      0.61       207\n",
      "           4       0.82      0.76      0.79       217\n",
      "           5       0.60      0.51      0.55       179\n",
      "           6       0.86      0.67      0.75       178\n",
      "           7       0.82      0.78      0.80       205\n",
      "           8       0.63      0.52      0.57       192\n",
      "           9       0.75      0.85      0.79       194\n",
      "\n",
      "    accuracy                           0.74      2000\n",
      "   macro avg       0.74      0.74      0.74      2000\n",
      "weighted avg       0.74      0.74      0.74      2000\n",
      "\n",
      "SELECTED sample:  4977\n",
      "SELECTED sample:  446\n",
      "SELECTED sample:  1041\n",
      "SELECTED sample:  449\n",
      "SELECTED sample:  3662\n",
      "SELECTED sample:  1196\n",
      "SELECTED sample:  341\n",
      "SELECTED sample:  679\n",
      "SELECTED sample:  2340\n",
      "SELECTED sample:  2555\n",
      "SELECTED sample:  2128\n",
      "SELECTED sample:  2956\n",
      "SELECTED sample:  969\n",
      "SELECTED sample:  2732\n",
      "SELECTED sample:  3412\n",
      "SELECTED sample:  2664\n",
      "SELECTED sample:  3756\n",
      "SELECTED sample:  930\n",
      "SELECTED sample:  2628\n",
      "SELECTED sample:  606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.97      0.88       175\n",
      "           1       0.90      0.98      0.94       234\n",
      "           2       0.84      0.71      0.77       219\n",
      "           3       0.63      0.78      0.70       207\n",
      "           4       0.79      0.70      0.74       217\n",
      "           5       0.75      0.59      0.66       179\n",
      "           6       0.81      0.73      0.77       178\n",
      "           7       0.81      0.86      0.83       205\n",
      "           8       0.71      0.61      0.66       192\n",
      "           9       0.70      0.78      0.74       194\n",
      "\n",
      "    accuracy                           0.78      2000\n",
      "   macro avg       0.77      0.77      0.77      2000\n",
      "weighted avg       0.78      0.78      0.77      2000\n",
      "\n",
      "SELECTED sample:  4977\n",
      "SELECTED sample:  446\n",
      "SELECTED sample:  1041\n",
      "SELECTED sample:  449\n",
      "SELECTED sample:  3662\n",
      "SELECTED sample:  1196\n",
      "SELECTED sample:  341\n",
      "SELECTED sample:  679\n",
      "SELECTED sample:  2340\n",
      "SELECTED sample:  2555\n",
      "SELECTED sample:  2128\n",
      "SELECTED sample:  2956\n",
      "SELECTED sample:  969\n",
      "SELECTED sample:  2732\n",
      "SELECTED sample:  3412\n",
      "SELECTED sample:  2664\n",
      "SELECTED sample:  3756\n",
      "SELECTED sample:  930\n",
      "SELECTED sample:  2628\n",
      "SELECTED sample:  606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.94      0.86       175\n",
      "           1       0.88      0.99      0.93       234\n",
      "           2       0.79      0.64      0.71       219\n",
      "           3       0.65      0.85      0.73       207\n",
      "           4       0.81      0.72      0.76       217\n",
      "           5       0.78      0.60      0.68       179\n",
      "           6       0.86      0.71      0.78       178\n",
      "           7       0.79      0.82      0.80       205\n",
      "           8       0.70      0.60      0.64       192\n",
      "           9       0.68      0.76      0.72       194\n",
      "\n",
      "    accuracy                           0.77      2000\n",
      "   macro avg       0.77      0.76      0.76      2000\n",
      "weighted avg       0.77      0.77      0.76      2000\n",
      "\n",
      "SELECTED sample:  4977\n",
      "SELECTED sample:  446\n",
      "SELECTED sample:  1041\n",
      "SELECTED sample:  449\n",
      "SELECTED sample:  3662\n",
      "SELECTED sample:  1196\n",
      "SELECTED sample:  341\n",
      "SELECTED sample:  679\n",
      "SELECTED sample:  2340\n",
      "SELECTED sample:  2555\n",
      "SELECTED sample:  2128\n",
      "SELECTED sample:  2956\n",
      "SELECTED sample:  969\n",
      "SELECTED sample:  2732\n",
      "SELECTED sample:  3412\n",
      "SELECTED sample:  2664\n",
      "SELECTED sample:  3756\n",
      "SELECTED sample:  930\n",
      "SELECTED sample:  2628\n",
      "SELECTED sample:  606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.94      0.86       175\n",
      "           1       0.89      0.98      0.93       234\n",
      "           2       0.82      0.73      0.77       219\n",
      "           3       0.66      0.83      0.73       207\n",
      "           4       0.87      0.74      0.80       217\n",
      "           5       0.78      0.64      0.70       179\n",
      "           6       0.89      0.74      0.81       178\n",
      "           7       0.79      0.80      0.80       205\n",
      "           8       0.71      0.61      0.66       192\n",
      "           9       0.71      0.81      0.76       194\n",
      "\n",
      "    accuracy                           0.79      2000\n",
      "   macro avg       0.79      0.78      0.78      2000\n",
      "weighted avg       0.79      0.79      0.78      2000\n",
      "\n",
      "SELECTED sample:  4977\n",
      "SELECTED sample:  446\n",
      "SELECTED sample:  1041\n",
      "SELECTED sample:  449\n",
      "SELECTED sample:  3662\n",
      "SELECTED sample:  1196\n",
      "SELECTED sample:  341\n",
      "SELECTED sample:  679\n",
      "SELECTED sample:  2340\n",
      "SELECTED sample:  2555\n",
      "SELECTED sample:  2128\n",
      "SELECTED sample:  2956\n",
      "SELECTED sample:  969\n",
      "SELECTED sample:  2732\n",
      "SELECTED sample:  3412\n",
      "SELECTED sample:  2664\n",
      "SELECTED sample:  3756\n",
      "SELECTED sample:  930\n",
      "SELECTED sample:  2628\n",
      "SELECTED sample:  606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.95      0.86       175\n",
      "           1       0.85      0.98      0.91       234\n",
      "           2       0.84      0.76      0.80       219\n",
      "           3       0.71      0.80      0.75       207\n",
      "           4       0.88      0.79      0.83       217\n",
      "           5       0.77      0.71      0.74       179\n",
      "           6       0.82      0.69      0.75       178\n",
      "           7       0.81      0.84      0.82       205\n",
      "           8       0.76      0.63      0.69       192\n",
      "           9       0.77      0.83      0.80       194\n",
      "\n",
      "    accuracy                           0.80      2000\n",
      "   macro avg       0.80      0.80      0.80      2000\n",
      "weighted avg       0.80      0.80      0.80      2000\n",
      "\n",
      "SELECTED sample:  4977\n",
      "SELECTED sample:  446\n",
      "SELECTED sample:  1041\n",
      "SELECTED sample:  449\n",
      "SELECTED sample:  3662\n",
      "SELECTED sample:  1196\n",
      "SELECTED sample:  341\n",
      "SELECTED sample:  679\n",
      "SELECTED sample:  2340\n",
      "SELECTED sample:  2555\n",
      "SELECTED sample:  2128\n",
      "SELECTED sample:  2956\n",
      "SELECTED sample:  969\n",
      "SELECTED sample:  2732\n",
      "SELECTED sample:  3412\n",
      "SELECTED sample:  2664\n",
      "SELECTED sample:  3756\n",
      "SELECTED sample:  930\n",
      "SELECTED sample:  2628\n",
      "SELECTED sample:  606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.95      0.86       175\n",
      "           1       0.83      0.96      0.89       234\n",
      "           2       0.81      0.74      0.77       219\n",
      "           3       0.67      0.81      0.73       207\n",
      "           4       0.85      0.75      0.80       217\n",
      "           5       0.75      0.69      0.72       179\n",
      "           6       0.87      0.66      0.75       178\n",
      "           7       0.81      0.81      0.81       205\n",
      "           8       0.76      0.60      0.67       192\n",
      "           9       0.74      0.83      0.78       194\n",
      "\n",
      "    accuracy                           0.78      2000\n",
      "   macro avg       0.79      0.78      0.78      2000\n",
      "weighted avg       0.79      0.78      0.78      2000\n",
      "\n",
      "SELECTED sample:  4977\n",
      "SELECTED sample:  446\n",
      "SELECTED sample:  1041\n",
      "SELECTED sample:  449\n",
      "SELECTED sample:  3662\n",
      "SELECTED sample:  1196\n",
      "SELECTED sample:  341\n",
      "SELECTED sample:  679\n",
      "SELECTED sample:  2340\n",
      "SELECTED sample:  2555\n",
      "SELECTED sample:  2128\n",
      "SELECTED sample:  2956\n",
      "SELECTED sample:  969\n",
      "SELECTED sample:  2732\n",
      "SELECTED sample:  3412\n",
      "SELECTED sample:  2664\n",
      "SELECTED sample:  3756\n",
      "SELECTED sample:  930\n",
      "SELECTED sample:  2628\n",
      "SELECTED sample:  606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.38      0.25       175\n",
      "           1       0.36      0.49      0.42       234\n",
      "           2       0.25      0.33      0.29       219\n",
      "           3       0.26      0.33      0.29       207\n",
      "           4       0.40      0.40      0.40       217\n",
      "           5       0.26      0.15      0.19       179\n",
      "           6       0.37      0.26      0.30       178\n",
      "           7       0.42      0.23      0.30       205\n",
      "           8       0.38      0.19      0.26       192\n",
      "           9       0.50      0.30      0.37       194\n",
      "\n",
      "    accuracy                           0.31      2000\n",
      "   macro avg       0.34      0.31      0.31      2000\n",
      "weighted avg       0.34      0.31      0.31      2000\n",
      "\n",
      "SELECTED sample:  4977\n",
      "SELECTED sample:  446\n",
      "SELECTED sample:  1041\n",
      "SELECTED sample:  449\n",
      "SELECTED sample:  3662\n",
      "SELECTED sample:  1196\n",
      "SELECTED sample:  341\n",
      "SELECTED sample:  679\n",
      "SELECTED sample:  2340\n",
      "SELECTED sample:  2555\n",
      "SELECTED sample:  2128\n",
      "SELECTED sample:  2956\n",
      "SELECTED sample:  969\n",
      "SELECTED sample:  2732\n",
      "SELECTED sample:  3412\n",
      "SELECTED sample:  2664\n",
      "SELECTED sample:  3756\n",
      "SELECTED sample:  930\n",
      "SELECTED sample:  2628\n",
      "SELECTED sample:  606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.34      0.18       175\n",
      "           1       0.20      0.30      0.24       234\n",
      "           2       0.12      0.15      0.13       219\n",
      "           3       0.14      0.15      0.14       207\n",
      "           4       0.14      0.14      0.14       217\n",
      "           5       0.11      0.05      0.07       179\n",
      "           6       0.25      0.13      0.17       178\n",
      "           7       0.15      0.08      0.10       205\n",
      "           8       0.18      0.07      0.10       192\n",
      "           9       0.13      0.04      0.06       194\n",
      "\n",
      "    accuracy                           0.15      2000\n",
      "   macro avg       0.15      0.15      0.13      2000\n",
      "weighted avg       0.15      0.15      0.14      2000\n",
      "\n",
      "SELECTED sample:  4977\n",
      "SELECTED sample:  446\n",
      "SELECTED sample:  1041\n",
      "SELECTED sample:  449\n",
      "SELECTED sample:  3662\n",
      "SELECTED sample:  1196\n",
      "SELECTED sample:  341\n",
      "SELECTED sample:  679\n",
      "SELECTED sample:  2340\n",
      "SELECTED sample:  2555\n",
      "SELECTED sample:  2128\n",
      "SELECTED sample:  2956\n",
      "SELECTED sample:  969\n",
      "SELECTED sample:  2732\n",
      "SELECTED sample:  3412\n",
      "SELECTED sample:  2664\n",
      "SELECTED sample:  3756\n",
      "SELECTED sample:  930\n",
      "SELECTED sample:  2628\n",
      "SELECTED sample:  606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.20      0.11       175\n",
      "           1       0.11      0.18      0.14       234\n",
      "           2       0.13      0.20      0.15       219\n",
      "           3       0.12      0.14      0.13       207\n",
      "           4       0.09      0.07      0.08       217\n",
      "           5       0.13      0.07      0.09       179\n",
      "           6       0.15      0.08      0.11       178\n",
      "           7       0.12      0.05      0.07       205\n",
      "           8       0.09      0.03      0.04       192\n",
      "           9       0.11      0.04      0.05       194\n",
      "\n",
      "    accuracy                           0.11      2000\n",
      "   macro avg       0.11      0.11      0.10      2000\n",
      "weighted avg       0.11      0.11      0.10      2000\n",
      "\n",
      "SELECTED sample:  4977\n",
      "SELECTED sample:  446\n",
      "SELECTED sample:  1041\n",
      "SELECTED sample:  449\n",
      "SELECTED sample:  3662\n",
      "SELECTED sample:  1196\n",
      "SELECTED sample:  341\n",
      "SELECTED sample:  679\n",
      "SELECTED sample:  2340\n",
      "SELECTED sample:  2555\n",
      "SELECTED sample:  2128\n",
      "SELECTED sample:  2956\n",
      "SELECTED sample:  969\n",
      "SELECTED sample:  2732\n",
      "SELECTED sample:  3412\n",
      "SELECTED sample:  2664\n",
      "SELECTED sample:  3756\n",
      "SELECTED sample:  930\n",
      "SELECTED sample:  2628\n",
      "SELECTED sample:  606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.09      0.23      0.13       175\n",
      "           1       0.12      0.20      0.15       234\n",
      "           2       0.11      0.15      0.13       219\n",
      "           3       0.11      0.13      0.12       207\n",
      "           4       0.12      0.11      0.11       217\n",
      "           5       0.15      0.09      0.12       179\n",
      "           6       0.14      0.10      0.11       178\n",
      "           7       0.11      0.04      0.06       205\n",
      "           8       0.05      0.01      0.02       192\n",
      "           9       0.08      0.02      0.03       194\n",
      "\n",
      "    accuracy                           0.11      2000\n",
      "   macro avg       0.11      0.11      0.10      2000\n",
      "weighted avg       0.11      0.11      0.10      2000\n",
      "\n",
      "SELECTED sample:  4977\n",
      "SELECTED sample:  446\n",
      "SELECTED sample:  1041\n",
      "SELECTED sample:  449\n",
      "SELECTED sample:  3662\n",
      "SELECTED sample:  1196\n",
      "SELECTED sample:  341\n",
      "SELECTED sample:  679\n",
      "SELECTED sample:  2340\n",
      "SELECTED sample:  2555\n",
      "SELECTED sample:  2128\n",
      "SELECTED sample:  2956\n",
      "SELECTED sample:  969\n",
      "SELECTED sample:  2732\n",
      "SELECTED sample:  3412\n",
      "SELECTED sample:  2664\n",
      "SELECTED sample:  3756\n",
      "SELECTED sample:  930\n",
      "SELECTED sample:  2628\n",
      "SELECTED sample:  606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.25      0.14       175\n",
      "           1       0.10      0.17      0.13       234\n",
      "           2       0.08      0.11      0.09       219\n",
      "           3       0.07      0.10      0.08       207\n",
      "           4       0.11      0.10      0.10       217\n",
      "           5       0.10      0.06      0.07       179\n",
      "           6       0.11      0.04      0.06       178\n",
      "           7       0.06      0.02      0.04       205\n",
      "           8       0.07      0.03      0.04       192\n",
      "           9       0.04      0.02      0.02       194\n",
      "\n",
      "    accuracy                           0.09      2000\n",
      "   macro avg       0.08      0.09      0.08      2000\n",
      "weighted avg       0.08      0.09      0.08      2000\n",
      "\n",
      "SELECTED sample:  4977\n",
      "SELECTED sample:  446\n",
      "SELECTED sample:  1041\n",
      "SELECTED sample:  449\n",
      "SELECTED sample:  3662\n",
      "SELECTED sample:  1196\n",
      "SELECTED sample:  341\n",
      "SELECTED sample:  679\n",
      "SELECTED sample:  2340\n",
      "SELECTED sample:  2555\n",
      "SELECTED sample:  2128\n",
      "SELECTED sample:  2956\n",
      "SELECTED sample:  969\n",
      "SELECTED sample:  2732\n",
      "SELECTED sample:  3412\n",
      "SELECTED sample:  2664\n",
      "SELECTED sample:  3756\n",
      "SELECTED sample:  930\n",
      "SELECTED sample:  2628\n",
      "SELECTED sample:  606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.23      0.12       175\n",
      "           1       0.13      0.22      0.16       234\n",
      "           2       0.13      0.18      0.15       219\n",
      "           3       0.11      0.13      0.12       207\n",
      "           4       0.09      0.08      0.09       217\n",
      "           5       0.08      0.04      0.06       179\n",
      "           6       0.07      0.04      0.05       178\n",
      "           7       0.08      0.02      0.04       205\n",
      "           8       0.08      0.02      0.03       192\n",
      "           9       0.09      0.03      0.05       194\n",
      "\n",
      "    accuracy                           0.10      2000\n",
      "   macro avg       0.09      0.10      0.09      2000\n",
      "weighted avg       0.10      0.10      0.09      2000\n",
      "\n",
      "SELECTED sample:  4977\n",
      "SELECTED sample:  446\n",
      "SELECTED sample:  1041\n",
      "SELECTED sample:  449\n",
      "SELECTED sample:  3662\n",
      "SELECTED sample:  1196\n",
      "SELECTED sample:  341\n",
      "SELECTED sample:  679\n",
      "SELECTED sample:  2340\n",
      "SELECTED sample:  2555\n",
      "SELECTED sample:  2128\n",
      "SELECTED sample:  2956\n",
      "SELECTED sample:  969\n",
      "SELECTED sample:  2732\n",
      "SELECTED sample:  3412\n",
      "SELECTED sample:  2664\n",
      "SELECTED sample:  3756\n",
      "SELECTED sample:  930\n",
      "SELECTED sample:  2628\n",
      "SELECTED sample:  606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.22      0.12       175\n",
      "           1       0.10      0.17      0.13       234\n",
      "           2       0.10      0.16      0.12       219\n",
      "           3       0.10      0.12      0.11       207\n",
      "           4       0.11      0.09      0.10       217\n",
      "           5       0.10      0.07      0.08       179\n",
      "           6       0.10      0.06      0.07       178\n",
      "           7       0.13      0.04      0.07       205\n",
      "           8       0.14      0.04      0.06       192\n",
      "           9       0.17      0.05      0.08       194\n",
      "\n",
      "    accuracy                           0.10      2000\n",
      "   macro avg       0.11      0.10      0.09      2000\n",
      "weighted avg       0.11      0.10      0.09      2000\n",
      "\n",
      "SELECTED sample:  4977\n",
      "SELECTED sample:  446\n",
      "SELECTED sample:  1041\n",
      "SELECTED sample:  449\n",
      "SELECTED sample:  3662\n",
      "SELECTED sample:  1196\n",
      "SELECTED sample:  341\n",
      "SELECTED sample:  679\n",
      "SELECTED sample:  2340\n",
      "SELECTED sample:  2555\n",
      "SELECTED sample:  2128\n",
      "SELECTED sample:  2956\n",
      "SELECTED sample:  969\n",
      "SELECTED sample:  2732\n",
      "SELECTED sample:  3412\n",
      "SELECTED sample:  2664\n",
      "SELECTED sample:  3756\n",
      "SELECTED sample:  930\n",
      "SELECTED sample:  2628\n",
      "SELECTED sample:  606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.19      0.11       175\n",
      "           1       0.12      0.23      0.15       234\n",
      "           2       0.11      0.15      0.12       219\n",
      "           3       0.11      0.12      0.11       207\n",
      "           4       0.11      0.09      0.10       217\n",
      "           5       0.09      0.05      0.06       179\n",
      "           6       0.11      0.06      0.07       178\n",
      "           7       0.10      0.03      0.05       205\n",
      "           8       0.10      0.02      0.03       192\n",
      "           9       0.13      0.04      0.06       194\n",
      "\n",
      "    accuracy                           0.10      2000\n",
      "   macro avg       0.10      0.10      0.09      2000\n",
      "weighted avg       0.10      0.10      0.09      2000\n",
      "\n",
      "SELECTED sample:  4977\n",
      "SELECTED sample:  446\n",
      "SELECTED sample:  1041\n",
      "SELECTED sample:  449\n",
      "SELECTED sample:  3662\n",
      "SELECTED sample:  1196\n",
      "SELECTED sample:  341\n",
      "SELECTED sample:  679\n",
      "SELECTED sample:  2340\n",
      "SELECTED sample:  2555\n",
      "SELECTED sample:  2128\n",
      "SELECTED sample:  2956\n",
      "SELECTED sample:  969\n",
      "SELECTED sample:  2732\n",
      "SELECTED sample:  3412\n",
      "SELECTED sample:  2664\n",
      "SELECTED sample:  3756\n",
      "SELECTED sample:  930\n",
      "SELECTED sample:  2628\n",
      "SELECTED sample:  606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.28      0.15       175\n",
      "           1       0.13      0.23      0.17       234\n",
      "           2       0.11      0.16      0.13       219\n",
      "           3       0.09      0.10      0.10       207\n",
      "           4       0.08      0.06      0.07       217\n",
      "           5       0.10      0.06      0.07       179\n",
      "           6       0.08      0.05      0.06       178\n",
      "           7       0.07      0.02      0.04       205\n",
      "           8       0.06      0.02      0.03       192\n",
      "           9       0.02      0.01      0.01       194\n",
      "\n",
      "    accuracy                           0.10      2000\n",
      "   macro avg       0.08      0.10      0.08      2000\n",
      "weighted avg       0.09      0.10      0.08      2000\n",
      "\n",
      "SELECTED sample:  4977\n",
      "SELECTED sample:  446\n",
      "SELECTED sample:  1041\n",
      "SELECTED sample:  449\n",
      "SELECTED sample:  3662\n",
      "SELECTED sample:  1196\n",
      "SELECTED sample:  341\n",
      "SELECTED sample:  679\n",
      "SELECTED sample:  2340\n",
      "SELECTED sample:  2555\n",
      "SELECTED sample:  2128\n",
      "SELECTED sample:  2956\n",
      "SELECTED sample:  969\n",
      "SELECTED sample:  2732\n",
      "SELECTED sample:  3412\n",
      "SELECTED sample:  2664\n",
      "SELECTED sample:  3756\n",
      "SELECTED sample:  930\n",
      "SELECTED sample:  2628\n",
      "SELECTED sample:  606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.09      0.23      0.13       175\n",
      "           1       0.10      0.19      0.13       234\n",
      "           2       0.10      0.14      0.12       219\n",
      "           3       0.12      0.13      0.12       207\n",
      "           4       0.12      0.12      0.12       217\n",
      "           5       0.08      0.04      0.05       179\n",
      "           6       0.12      0.07      0.09       178\n",
      "           7       0.12      0.04      0.06       205\n",
      "           8       0.09      0.03      0.04       192\n",
      "           9       0.19      0.05      0.07       194\n",
      "\n",
      "    accuracy                           0.10      2000\n",
      "   macro avg       0.11      0.10      0.09      2000\n",
      "weighted avg       0.11      0.10      0.10      2000\n",
      "\n",
      "SELECTED sample:  4977\n",
      "SELECTED sample:  446\n",
      "SELECTED sample:  1041\n",
      "SELECTED sample:  449\n",
      "SELECTED sample:  3662\n",
      "SELECTED sample:  1196\n",
      "SELECTED sample:  341\n",
      "SELECTED sample:  679\n",
      "SELECTED sample:  2340\n",
      "SELECTED sample:  2555\n",
      "SELECTED sample:  2128\n",
      "SELECTED sample:  2956\n",
      "SELECTED sample:  969\n",
      "SELECTED sample:  2732\n",
      "SELECTED sample:  3412\n",
      "SELECTED sample:  2664\n",
      "SELECTED sample:  3756\n",
      "SELECTED sample:  930\n",
      "SELECTED sample:  2628\n",
      "SELECTED sample:  606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.09      0.25      0.14       175\n",
      "           1       0.12      0.25      0.17       234\n",
      "           2       0.13      0.16      0.14       219\n",
      "           3       0.11      0.13      0.12       207\n",
      "           4       0.10      0.09      0.10       217\n",
      "           5       0.11      0.06      0.08       179\n",
      "           6       0.06      0.03      0.04       178\n",
      "           7       0.11      0.04      0.06       205\n",
      "           8       0.12      0.03      0.05       192\n",
      "           9       0.11      0.03      0.04       194\n",
      "\n",
      "    accuracy                           0.11      2000\n",
      "   macro avg       0.11      0.11      0.09      2000\n",
      "weighted avg       0.11      0.11      0.10      2000\n",
      "\n",
      "SELECTED sample:  4977\n",
      "SELECTED sample:  446\n",
      "SELECTED sample:  1041\n",
      "SELECTED sample:  449\n",
      "SELECTED sample:  3662\n",
      "SELECTED sample:  1196\n",
      "SELECTED sample:  341\n",
      "SELECTED sample:  679\n",
      "SELECTED sample:  2340\n",
      "SELECTED sample:  2555\n",
      "SELECTED sample:  2128\n",
      "SELECTED sample:  2956\n",
      "SELECTED sample:  969\n",
      "SELECTED sample:  2732\n",
      "SELECTED sample:  3412\n",
      "SELECTED sample:  2664\n",
      "SELECTED sample:  3756\n",
      "SELECTED sample:  930\n",
      "SELECTED sample:  2628\n",
      "SELECTED sample:  606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.21      0.11       175\n",
      "           1       0.12      0.23      0.16       234\n",
      "           2       0.13      0.18      0.15       219\n",
      "           3       0.08      0.09      0.08       207\n",
      "           4       0.07      0.06      0.07       217\n",
      "           5       0.06      0.03      0.04       179\n",
      "           6       0.10      0.06      0.07       178\n",
      "           7       0.10      0.03      0.05       205\n",
      "           8       0.05      0.01      0.02       192\n",
      "           9       0.06      0.02      0.02       194\n",
      "\n",
      "    accuracy                           0.09      2000\n",
      "   macro avg       0.08      0.09      0.08      2000\n",
      "weighted avg       0.09      0.09      0.08      2000\n",
      "\n",
      "SELECTED sample:  4977\n",
      "SELECTED sample:  446\n",
      "SELECTED sample:  1041\n",
      "SELECTED sample:  449\n",
      "SELECTED sample:  3662\n",
      "SELECTED sample:  1196\n",
      "SELECTED sample:  341\n",
      "SELECTED sample:  679\n",
      "SELECTED sample:  2340\n",
      "SELECTED sample:  2555\n",
      "SELECTED sample:  2128\n",
      "SELECTED sample:  2956\n",
      "SELECTED sample:  969\n",
      "SELECTED sample:  2732\n",
      "SELECTED sample:  3412\n",
      "SELECTED sample:  2664\n",
      "SELECTED sample:  3756\n",
      "SELECTED sample:  930\n",
      "SELECTED sample:  2628\n",
      "SELECTED sample:  606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.09      0.22      0.12       175\n",
      "           1       0.13      0.25      0.17       234\n",
      "           2       0.12      0.16      0.14       219\n",
      "           3       0.14      0.14      0.14       207\n",
      "           4       0.08      0.06      0.07       217\n",
      "           5       0.05      0.03      0.04       179\n",
      "           6       0.10      0.06      0.07       178\n",
      "           7       0.12      0.06      0.08       205\n",
      "           8       0.07      0.02      0.03       192\n",
      "           9       0.21      0.06      0.09       194\n",
      "\n",
      "    accuracy                           0.11      2000\n",
      "   macro avg       0.11      0.11      0.09      2000\n",
      "weighted avg       0.11      0.11      0.10      2000\n",
      "\n",
      "SELECTED sample:  4977\n",
      "SELECTED sample:  446\n",
      "SELECTED sample:  1041\n",
      "SELECTED sample:  449\n",
      "SELECTED sample:  3662\n",
      "SELECTED sample:  1196\n",
      "SELECTED sample:  341\n",
      "SELECTED sample:  679\n",
      "SELECTED sample:  2340\n",
      "SELECTED sample:  2555\n",
      "SELECTED sample:  2128\n",
      "SELECTED sample:  2956\n",
      "SELECTED sample:  969\n",
      "SELECTED sample:  2732\n",
      "SELECTED sample:  3412\n",
      "SELECTED sample:  2664\n",
      "SELECTED sample:  3756\n",
      "SELECTED sample:  930\n",
      "SELECTED sample:  2628\n",
      "SELECTED sample:  606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.29      0.15       175\n",
      "           1       0.13      0.24      0.17       234\n",
      "           2       0.12      0.16      0.13       219\n",
      "           3       0.10      0.10      0.10       207\n",
      "           4       0.11      0.09      0.10       217\n",
      "           5       0.06      0.03      0.04       179\n",
      "           6       0.06      0.03      0.04       178\n",
      "           7       0.09      0.03      0.05       205\n",
      "           8       0.06      0.02      0.02       192\n",
      "           9       0.07      0.03      0.04       194\n",
      "\n",
      "    accuracy                           0.10      2000\n",
      "   macro avg       0.09      0.10      0.08      2000\n",
      "weighted avg       0.09      0.10      0.09      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "levels=20\n",
    "nodes=100\n",
    "sim_bases=0.001#15\n",
    "sim_baseVSacc=[]\n",
    "while(sim_bases<=0.1):\n",
    "    np.random.seed(0)\n",
    "    seed,bases,CodeBooks=HyperSeed_2(TR_perm,TR_L_perm,TE_perm,TE_L_perm,sim_bases,levels,nodes,d)\n",
    "    trainData, trainLabels= [],[]\n",
    "    samples=[]\n",
    "    results=[]\n",
    "    stats= np.zeros((levels,nodes))\n",
    "    for i in range(len(TR_perm)):\n",
    "        hd_new = TR_perm[i];\n",
    "        Ibmu = seed * np.conj(hd_new);\n",
    "        inds,work=findCoordinates(Ibmu,CodeBooks)\n",
    "        for j in range(len(inds)):\n",
    "            stats[j][int(inds[j])]+=1\n",
    "        trainData.append(inds.astype('int'))\n",
    "        samples.append(hd_new)\n",
    "        results.append(inds.astype('int'))\n",
    "        trainLabels.append(TR_L_perm[i])\n",
    "    trainData= np.array(trainData).real\n",
    "\n",
    "    testData, testLabels= [],[]\n",
    "    for i in range(len(TE_perm)):\n",
    "        hd_new = TE_perm[i];\n",
    "        Ibmu = seed * np.conj(hd_new);\n",
    "        inds,work=findCoordinates(Ibmu,CodeBooks)\n",
    "        testData.append(inds.astype('int'))\n",
    "        testLabels.append(TE_L_perm[i])\n",
    "    testData=np.array(testData).real\n",
    "    trainLabels=np.array(trainLabels)\n",
    "    testLabels=np.array(testLabels)\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn import svm\n",
    "\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "    model = KNeighborsClassifier(n_neighbors=3)# LogisticRegression(random_state=0)\n",
    "    # model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "    # LR_classifier.fit(trainData, trainLabels)\n",
    "    model.fit(trainData, trainLabels-1)\n",
    "    prediction = model.predict(testData)\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(classification_report(testLabels-1, prediction))\n",
    "    acc=np.round(classification_report(testLabels-1, prediction,output_dict=True)[\"accuracy\"],3)\n",
    "    sim_baseVSacc.append([sim_bases,acc])\n",
    "    sim_bases+=0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028d0d66-dcba-4e92-aa4a-89809760e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 14})# stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eda4a516-2fc6-4a21-b349-8b4f7e8e0891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data to be plotted\n",
    "x = np.array(sim_baseVSacc)[:,0]\n",
    "y = np.array(sim_baseVSacc)[:,1]\n",
    "\n",
    "# plotting \n",
    "# plt.title(\"Sim_bases vs Acc\")\n",
    "plt.xlabel(\"Sim_base\")\n",
    "plt.ylabel(\"Acc\")\n",
    "plt.plot(x, y, color =\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68744dd-7dd3-4182-b611-48b8bb4fcb09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd]",
   "language": "python",
   "name": "conda-env-phd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
